{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data = pd.read_csv('./../processed_VideoCommentsThreatCorpus.csv')\n",
    "\n",
    "pred_data = pd.read_csv('batch_2025-03-25_13-46.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_eval(true_df: pd.DataFrame, pred_df: pd.DataFrame):\n",
    "    # Ensure agent_flags values are binary integers\n",
    "    pred_df['violence_label'] = pred_df['violence_label'].apply(lambda x: 0 if x in [0, 1] else 1)\n",
    "    \n",
    "    # Merge datasets on id and document_id\n",
    "    merged_df = true_df.merge(pred_df, left_on='id', right_on='document_id', suffixes=('_true', '_pred'))\n",
    "\n",
    "    # Convert string lists like \"[0]\" or \"[1]\" into actual integers\n",
    "    merged_df['flags'] = merged_df['flags'].astype(str).str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "    # Extract true and predicted labels\n",
    "    y_true = merged_df['flags']\n",
    "    y_pred = merged_df['violence_label']\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "\n",
    "    # Print results\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print('\\nConfusion Matrix:\\n', cm)\n",
    "    print('\\nClassification Report:\\n', report)\n",
    "\n",
    "    # Visualization of Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Violent', 'Violent'], yticklabels=['Non-Violent', 'Violent'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot distribution of true and predicted labels\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.countplot(x=y_true, ax=ax[0], palette='viridis')\n",
    "    ax[0].set_title('True Label Distribution')\n",
    "    ax[0].set_xticklabels(['Non-Violent', 'Violent'])\n",
    "\n",
    "    sns.countplot(x=y_pred, ax=ax[1], palette='magma')\n",
    "    ax[1].set_title('Predicted Label Distribution')\n",
    "    ax[1].set_xticklabels(['Non-Violent', 'Violent'])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "batch_eval(true_data, pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in agent_flags:\", pred_data['agent_flags'].unique())\n",
    "\n",
    "# Count occurrences of each unique value\n",
    "print(\"Value counts in agent_flags:\")\n",
    "print(pred_data['agent_flags'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
